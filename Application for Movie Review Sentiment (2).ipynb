{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [23/Mar/2020 11:17:40] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:17:49] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:17:49] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:17:49] \"\u001b[37mGET /favicon.ico HTTP/1.1\u001b[0m\" 200 -\n",
      "C:\\Users\\SpirosAvgoustatos\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\Users\\SpirosAvgoustatos\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator CountVectorizer from version 0.20.1 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "127.0.0.1 - - [23/Mar/2020 11:17:51] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:18:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:19:14] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2020 11:19:15] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from dash.dependencies import Input, Output\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import plotly.plotly as py\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Input(id='my-id', value='initial value', type='text'),\n",
    "    html.Div(id='my-div')\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='my-div', component_property='children'),\n",
    "    [Input(component_id='my-id', component_property='value')]\n",
    ")\n",
    "def update_output_div(input_value):\n",
    "    \n",
    "    filename = 'Logregtfidf.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(x) :\n",
    "        stop_words= set(stopwords.words(\"english\"))\n",
    "        count=0\n",
    "        for sentence in x : \n",
    "            sentence = [word for word in sentence.lower().split() if word not in stop_words]\n",
    "            sentence=' '.join(sentence)\n",
    "            x.loc[count]=sentence\n",
    "            count+=1\n",
    "        return(x)\n",
    "    \n",
    "    def remove_punctuation(x):\n",
    "        count = 0\n",
    "        for s in x:\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            s = re.sub(r'\\d+', '', s)\n",
    "            s = re.sub(cleanr, '', s)\n",
    "            s = re.sub(\"'\", '', s)\n",
    "            s = re.sub(r'\\W+', ' ', s)\n",
    "            s = s.replace('_', '')\n",
    "            x.loc[count] = s\n",
    "            count+=1\n",
    "        return(x)\n",
    "    \n",
    "    def lemma(df):\n",
    "\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        count = 0\n",
    "        stemmed = []\n",
    "        for sentence in df:    \n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            for word in word_tokens:\n",
    "                stemmed.append(lmtzr.lemmatize(word))\n",
    "            sentence = ' '.join(stemmed)\n",
    "            df.iloc[count] = sentence\n",
    "            count+=1\n",
    "            stemmed = []\n",
    "        return(df)\n",
    "\n",
    "    def stemma(df):\n",
    "\n",
    "        stemmer = SnowballStemmer(\"english\") #SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "        count = 0\n",
    "        stemmed = []\n",
    "        for sentence in df:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            for word in word_tokens:\n",
    "                stemmed.append(stemmer.stem(word))\n",
    "            sentence = ' '.join(stemmed)\n",
    "            df.iloc[count] = sentence\n",
    "            count+=1\n",
    "            stemmed = []\n",
    "        return(df)\n",
    "\n",
    "   \n",
    "    test = pd.DataFrame(data=[input_value])\n",
    "    test_x=remove_stopwords(test[0])\n",
    "    test_x=remove_punctuation(test_x)\n",
    "    test_x=lemma(test_x)\n",
    "    test_x=stemma(test_x)\n",
    "    cv = TfidfVectorizer(analyzer=\"word\")\n",
    "    tf1 = pickle.load(open(\"tfidf1.pkl\", 'rb'))\n",
    "    tf1_new = TfidfVectorizer(analyzer='word', vocabulary = tf1.vocabulary_)\n",
    "    x_test = tf1_new.fit_transform(test_x)\n",
    "    preds=loaded_model.predict(x_test)\n",
    "    predictions = np.array(preds)\n",
    "    dataset = pd.DataFrame({'predictions': predictions}, columns=['predictions'])\n",
    "    dataset[dataset[\"predictions\"]==1]=\"Positive !!!!\"\n",
    "    dataset[dataset[\"predictions\"]==0]=\"Negative !!!!\"\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return 'The Review you entered in the app is .... \"{}\"'.format(dataset[\"predictions\"].iloc[0]) \n",
    "#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
